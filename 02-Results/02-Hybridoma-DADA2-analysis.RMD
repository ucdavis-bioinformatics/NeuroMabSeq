---
title:  '`r paste0("Hybridoma Amplicon Report : ", plate)`'
author: "Sam Hunter"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = '/bio/CoreWork/2019.11.18-Trimmer-Hybridoma-Seq/2020-02-07-run-rerun_of_27-samples-SMARTPCR')
library(dada2)
library(kableExtra)
library(ggplot2)
library(stringr)
library(msaR)
library(DECIPHER)
options(stringsAsFactors=F)

# minReadSupport = .2  # only ASVs with at least this fraction of reads will be output
# minReadCount = 10  # only samples with at least this many reads will be processed
```

### Setup File and Sample Names

```{r Load Data}
# Load plate translation:
outp = './'
#sampleIDs = read.table("../samplesheet.tsv", header=T, as.is=T, sep='\t')

# submission = '9468dd86bf48'
# plate = "TRIMMER0004.2_P4"

## Load sample data from submission system:
# sampleData = read.table(paste("https://submissions.dnatech.ucdavis.edu/server/api/submissions/", 
#                               submission, "/download/?format=tsv&data=samples", sep=''), sep='\t', header=T, as.is=T)

sampleData = read.table(paste("https://ucdavis.coreomics.com/server/api/submissions/", 
                              submission, "/download/?format=tsv&data=samples", sep=''), sep='\t', header=T, as.is=T)

#sampleData = read.table('https://ucdavis.coreomics.com/server/api/submissions/b51b81e9ee80/download/?format=tsv&data=samples', sep='\t', header=T, as.is=T)


#https://ucdavis.coreomics.com/server/api/submissions/b51b81e9ee80/download/?format=tsv&data=samples


# Load SMARTindex --> Well ID conversion:
wellIDX = read.table("SMARTindex_well.tsv", head=T, as.is=T)
rownames(wellIDX) = wellIDX$index_name
P = strsplit(plate, '_|-')[[1]][2]
wellIDX$sample_name = paste(P, wellIDX$well, sep='_')

# Enforce plate name passed to this script. The information from the submission system (sampleData) can be wrong:
sampleData$sample_name = paste(P, sampleData$plate_location, sep='_')

#path = "../01-PrimerTrim-keep-aberrant-LC/"
path = "../01-PrimerTrim/"

list.files(path, pattern = "*_SE.fastq.gz")

# Get list of files
hcfiles = sort(list.files(path, pattern="*_HCprimers_SE.fastq.gz", full.names=T))
lcfiles = sort(list.files(path, pattern="*_LCprimers_SE.fastq.gz", full.names=T))

# Get SMARTPCR index identifier:
hcindexes = str_remove(basename(hcfiles), "_HCprimers_SE.fastq.gz")
lcindexes = str_remove(basename(lcfiles), "_LCprimers_SE.fastq.gz")

```

```{r learningErrors}
# Learn Error Rates
errHC = learnErrors(hcfiles, multithread = TRUE)
errLC = learnErrors(lcfiles, multithread = TRUE)
gc()

```

Plot error rates:

```{r Plot HC Error Model}
plotErrors(errHC, nominalQ = TRUE)

```

```{r Plot LC Error Models}
plotErrors(errLC, nominalQ = TRUE)

```


### Infer ASVs

```{r InferASV}

sp0 = "CAGCATCCTCTCTTCCAGCTCTCAGAGATGGAGACAGACACACTCCTGTTATGGGTACTGCTGCTCTGGG
TTCCAGGTTCCACTGGTGACATTGTGCTGACACAGTCTCCTGCTTCCTTAGCTGTATCTCTGGGGCAGAG
GGCCACCATCTCATACAGGGCCAGCAAAAGTGTCAGTACATCTGGCTATAGTTATATGCACTGGAACCAA
CAGAAACCAGGACAGCCACCCAGACTCCTCATCTATCTTGTATCCAACCTAGAATCTGGGGTCCCTGCCA
GGTTCAGTGGCAGTGGGTCTGGGACAGACTTCACCCTCAACATCCATCCTGTGGAGGAGGAGGATGCTGC
AACCTATTACTGTCAGCACATTAGGGAGCTTACACGTTCGGAGGGGGGACCAAGCTGGAAATAAAACGGG
CTGATGCTGCACCAACTGTATCCA"
sp0 = gsub('\n', '', sp0)

# Do denoising for HC:
dadaHC = dada(hcfiles, err=errHC, multithread=T, pool=T, priors=sp0, OMEGA_A=1e-60, OMEGA_C=0)
names(dadaHC) = hcindexes

# Do denoising for LC:
dadaLC = dada(lcfiles, err=errLC, multithread=T, pool=T, priors=sp0, OMEGA_A=1e-60, OMEGA_C=0)
names(dadaLC) = lcindexes

gc()

# Construct sequence tables
seqtabHC <- makeSequenceTable(dadaHC)
seqtabLC <- makeSequenceTable(dadaLC)

# Write some non-filtered tables for tracking and analysis
dim(seqtabHC)
seqtabHC.tab = data.frame(ASV=colnames(seqtabHC), t(seqtabHC))
seqtabHC.tab = seqtabHC.tab[order(rowSums(t(seqtabHC)), decreasing = T), ]
write.table(seqtabHC.tab, file='00-no-filter-seqtab_HC.tsv', sep='\t', row.names=F)

dim(seqtabLC)
seqtabLC.tab = data.frame(ASV=colnames(seqtabLC), t(seqtabLC))
seqtabLC.tab = seqtabLC.tab[order(rowSums(t(seqtabLC)), decreasing = T), ]
write.table(seqtabLC.tab, file='00-no-filter-seqtab_LC.tsv', sep='\t', row.names=F)

```

```{r ProcessingFunction}
# Align two sequences and calculate percent ID as number of matches / length of shorter sequence
pctid = function(s1,s2){
    ss = DNAStringSet(c(s1,s2))
    aln = AlignSeqs(ss, verbose=F)
    return(100*sum(as(aln, "matrix")[1,] == as(aln, "matrix")[2,])/min(width(ss)))
}

zeroCommon = function(m){
    # Drop ASVs present in >50 of samples with more than .05% of total reads (aberrant LC and/or contaminants)
    # The idea is that using a non-fixed minimum number of reads avoids dropping ASVs that are strongly present in one or
    # a few samples, but have been sequenced to really high depth with bleed into other samples. 
    #commonidx = colSums(m > 10)/nrow(m) < .5   # only keep ASVs that show up in < 50% of samples 
    m.zeroed = m
    commonidx = colSums(m > (.0005 * sum(m)))/nrow(m) > .5
    m.zeroed[,commonidx] = 0 #= m.zeroed[,commonidx]
    return(m.zeroed)
}


# Zero out every ASV that doesn't get at least 2% support in the ASV and at least 2% support in the sample.
# Rules:
#   1) Supporting read count must represent at least 2% of reads for that SAMPLE
#   2) Supporting read count must represent at least 2% of reads for that ASV
#   3) Supporting read count must be > 10
#   4) ASV may not have a read count > .05% of total reads in more than 1/2 of samples 
#   5) ASV must be < 90% identical to other ASVs from the sample ()
zeroMatrix = function(m){
  m.Sample.pct = 100*m/rowSums(zeroCommon(m))
  m.ASV.pct = t(100*t(m)/colSums(zeroCommon(m)))
  m.zeroed = m
  # Zero out anything that doesn't make the cut:
  for(i in 1:nrow(m.zeroed)){
      for(j in 1:ncol(m.zeroed)){
          if(m.ASV.pct[i,j] < 2 | m.Sample.pct[i,j] < 2 | m[i,j] < 10){
          m.zeroed[i,j] = 0
          }
      }
  }
  
  # Within each sample, compute an alignment and zero ASV counts for sequences that are very similar.
  for(i in 1:nrow(m.zeroed)){
      #seqcounts = m.zeroed[i,m.zeroed[i,]>0]
      seqcounts = m.zeroed[i,]
      if(sum(seqcounts > 0) > 1){ # don't process single ASV samples
          for(x in 1:(length(seqcounts)-1)){
              for(y in 2:length(seqcounts)){
                  if(m.zeroed[i,x] != 0 & m.zeroed[i,y] != 0 & x != y){
                  #if(seqcounts[x] != 0 & seqcounts[y] != 0 & x != y){
                      if(pctid(names(seqcounts)[x], names(seqcounts)[y]) > 90){
                          cat(paste0(c(paste(c(x,y), sep=','), '\n')))
                          loser = names(which(seqcounts[c(x,y)] == min(seqcounts[c(x,y)])))
                          m.zeroed[i,loser] = 0
                      }
                  }
              }
          }
      }
  }

  #Finally, zero out the common ASVs (contaminants/aberrant LC)
  m.zeroed = zeroCommon(m.zeroed)

  # Drop ASVs that are not present in any samples after zeroing:
  m.zeroed = m.zeroed[,colSums(m.zeroed)>0]
  return(m.zeroed)
}



# A function to convert seqtab results to a table:
asvToTable = function(st.filtered, st, sampleData, chain){
    options(stringsAsFactors = F)
    st.out = data.frame()
    for(s in rownames(st.filtered)){
        r = st.filtered[s, ]
        r = r[order(r, decreasing=T)]
        sample_name = wellIDX[s, 'sample_name']
        MabID = sampleData$trimmer_id[match(sample_name, sampleData$sample_name)]
        i=1
        for(asv in names(which(r > 0))){ 
            st.out = rbind(st.out, c(sample_name, plate, s, MabID, chain, 
                        paste0(sample_name, ".", chain, i), r[asv], signif(100*r[asv]/sum(st[s,]), 3), sum(st[s,]), "Illumina", asv))
            i = i + 1
        }
    }
    colnames(st.out) = c("Sample_Name", "plate", "SMARTindex", "MabID", "Chain", "ChainID", "ASVcount", "PctSupport", "TotalReads", "Sequencing", "ASV")
    # Find any ASVs reported more than once within the plate and note the duplicate
    st.out$DuplicatedIn = apply(st.out, 1, function(x){ paste(setdiff(st.out$Sample_Name[st.out$ASV %in% x['ASV']], x['Sample_Name']), collapse=', ')})
    return(st.out)
}

```


```{r Process ASVtables}

############ Filter ASVs and samples ######## 

# Filter and write HC:
seqtabHC.f1 = zeroCommon(seqtabHC)  # Used for no-abLC denominantor 
seqtabHC.f2 = zeroMatrix(seqtabHC)
readr::write_tsv(data.frame(t(seqtabHC.f2)), paste0(outp, "01-filtered_HC.tsv"))
HC.out = asvToTable(seqtabHC.f2, seqtabHC.f1, sampleData, "HC")
readr::write_tsv(HC.out, paste0(outp, plate, "_", "HeavyChain.tsv"))

# Filter and write LC:
seqtabLC.f1 = zeroCommon(seqtabLC)
seqtabLC.f2 = zeroMatrix(seqtabLC)
readr::write_tsv(data.frame(t(seqtabLC.f2)), paste0(outp, "01-filtered_LC.tsv"))
LC.out = asvToTable(seqtabLC.f2, seqtabLC.f1, sampleData, "LC")
readr::write_tsv(LC.out, paste0(outp, plate, "_", "LightChain.tsv"))

# Combine and write both
combined.out = rbind(HC.out, LC.out)
readr::write_tsv(combined.out, paste0(outp, plate, "_", "Sequences.tsv"))
```


### Putative aberrant LC alignment

Most common sequence in set:

```{r plotalignment1}
# Plot alignments of commonidxLC (are these aberrant?)
# So far haven't figure out how to do this in a loop, so just plot the one with the most total reads:
putative_aberrant = names(sort(colSums(seqtabLC), decreasing=T))[1]
dna = DNAStringSet(c(sp0, putative_aberrant))  
names(dna) = c("AberrantLC", "MostCommonSeq") 

alignedDNA = AlignSeqs(dna, verbose=F)

msaR(alignedDNA, menu=F, height=100, overviewbox = F, conservation=T, )

```
\
\
\

Second most common sequence in set:

```{r plotalignment2}
# Plot alignments of commonidxLC (are these aberrant?)
# So far haven't figure out how to do this in a loop, so just plot the one with the most total reads:
putative_aberrant = names(sort(colSums(seqtabLC), decreasing=T))[2]
dna = DNAStringSet(c(sp0, putative_aberrant))  
names(dna) = c("AberrantLC", "SecondMostCommonSeq") 

alignedDNA = AlignSeqs(dna, verbose=F)

msaR(alignedDNA, menu=F, height=100, overviewbox = F, conservation=T, )

```
\
\
\

```{r summary table}
## Build Status Table for all samples:
status.out = data.frame(sampleData,
                  LCs.Reported = as.vector(table(LC.out$Sample_Name)[sampleData$sample_name]),
                  HCs.Reported = as.vector(table(HC.out$Sample_Name)[sampleData$sample_name])
)

status.out$HCs.Reported[is.na(status.out$HCs.Reported)] = 0
status.out$LCs.Reported[is.na(status.out$LCs.Reported)] = 0

readr::write_tsv(status.out, paste0(outp, plate, "_", "SampleStatus.tsv"))

kable(status.out) %>%
  kable_styling(bootstrap_options = c("hover", "striped"), full_width = F, 
        position="left", fixed_thead=T) %>%
        row_spec(0, angle = 0)
```


