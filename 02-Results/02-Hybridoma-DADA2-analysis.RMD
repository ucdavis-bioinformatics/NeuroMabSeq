---
title: "Hybridoma Amplicon"
author: "SH"
date: "2/13/2020"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = '/bio/CoreWork/2019.11.18-Trimmer-Hybridoma-Seq/2020-02-07-run-rerun_of_27-samples-SMARTPCR')
library(dada2)
library(kableExtra)
library(ggplot2)
library(stringr)
```

### Setup File and Sample Names

```{r}
# Load plate translation:
outp = './'
sampleIDs = read.table("../samplesheet.tsv", header=T, as.is=T, sep='\t')

plate = "plate3"

#path = "../01-PrimerTrim-keep-aberrant-LC/"
path = "../01-PrimerTrim/"

list.files(path, pattern = "*_SE.fastq.gz")

# Get list of files
hcfiles = sort(list.files(path, pattern="*_10-REV-HC1_SE.fastq.gz", full.names=T))
lcfiles = sort(list.files(path, pattern="*_6-REV-LC_SE.fastq.gz", full.names=T))

# Get SMARTPCR index identifier:
hcindexes = str_remove(basename(hcfiles), "_10-REV-HC1_SE.fastq.gz")
lcindexes = str_remove(basename(lcfiles), "_6-REV-LC_SE.fastq.gz")

# Filter reads that have errors:
filtHC = file.path(path, 'filtered', paste0(hcindexes, "_HC_filt.fastq.gz"))
filtLC = file.path(path, 'filtered', paste0(lcindexes, "_LC_filt.fastq.gz"))

outHC = filterAndTrim(hcfiles, filtHC, maxEE = Inf, maxN=0, compress=TRUE, multithread=TRUE, truncQ=0)
gc()
outLC = filterAndTrim(lcfiles, filtLC, maxEE = 3, maxN=0, compress=TRUE, multithread=TRUE, truncQ=0)
gc()

# Learn Error Rates
errHC = learnErrors(filtHC, multithread = TRUE)
errLC = learnErrors(filtLC, multithread = TRUE)
gc()

```

Plot error rates:

```{r}
plotErrors(errHC, nominalQ = TRUE)

```

```{r}
plotErrors(errLC, nominalQ = TRUE)

```


### Infer ASVs

```{r}

sp0 = "CAGCATCCTCTCTTCCAGCTCTCAGAGATGGAGACAGACACACTCCTGTTATGGGTACTGCTGCTCTGGG
TTCCAGGTTCCACTGGTGACATTGTGCTGACACAGTCTCCTGCTTCCTTAGCTGTATCTCTGGGGCAGAG
GGCCACCATCTCATACAGGGCCAGCAAAAGTGTCAGTACATCTGGCTATAGTTATATGCACTGGAACCAA
CAGAAACCAGGACAGCCACCCAGACTCCTCATCTATCTTGTATCCAACCTAGAATCTGGGGTCCCTGCCA
GGTTCAGTGGCAGTGGGTCTGGGACAGACTTCACCCTCAACATCCATCCTGTGGAGGAGGAGGATGCTGC
AACCTATTACTGTCAGCACATTAGGGAGCTTACACGTTCGGAGGGGGGACCAAGCTGGAAATAAAACGGG
CTGATGCTGCACCAACTGTATCCA"

#dadaHC = dada(hcfiles, err=errHC, multithread=T, priors=sp0, OMEGA_A=1e-50, OMEGA_C=1, pool=T)
#dadaHC$`01-SMARTindex_10-REV-HC1_SE.fastq.gz`

# Do denoising for HC:
dadaHC = dada(filtHC, err=errHC, multithread=T, pool=T, priors=sp0, OMEGA_A=1e-60, OMEGA_C=0)
names(dadaHC) = hcindexes
#sapply(dadaHC, getN))

# Do denoising for LC:
dadaLC = dada(filtLC, err=errLC, multithread=T, pool=T, priors=sp0, OMEGA_A=1e-60, OMEGA_C=0)
names(dadaLC) = lcindexes
#sapply(dadaLC, getN)

#dadaLC$`01-SMARTindex_6-REV-LC_SE.fastq.gz`$denoised
gc()

# Construct sequence table
#seqtab <- makeSequenceTable(mergers)
seqtabHC <- makeSequenceTable(dadaHC)

seqtabLC <- makeSequenceTable(dadaLC)

# Write some non-filtered tables for tracking and analysis
dim(seqtabHC)
seqtabHC.tab = data.frame(ASV=colnames(seqtabHC), t(seqtabHC))
seqtabHC.tab = seqtabHC.tab[order(rowSums(t(seqtabHC)), decreasing = T), ]
write.table(seqtabHC.tab, file='00-no-filter-seqtab_HC.tsv', sep='\t', row.names=F)

dim(seqtabLC)
seqtabLC.tab = data.frame(ASV=colnames(seqtabLC), t(seqtabLC))
seqtabLC.tab = seqtabLC.tab[order(rowSums(t(seqtabLC)), decreasing = T), ]
write.table(seqtabLC.tab, file='00-no-filter-seqtab_LC.tsv', sep='\t', row.names=F)



############ Filter ASVs and samples ########
# Filters HC:
failedidxHC = rowSums(seqtabHC >= 10) > 0 # throw out samples that have no ASV with at least 10 reads of support 
commonidxHC = colSums(seqtabHC > 1)/nrow(seqtabHC) < .5   # throw out ASVs that show up in > 50% of samples 
seqtabHC.f = seqtabHC[failedidxHC, commonidxHC]
seqtabHC.f.pct = 100*seqtabHC.f/rowSums(seqtabHC.f)
lowcountidxHC = colSums(seqtabHC.f.pct > 30) > 0 # Throw out ASVs that do not get support from at least 30% of reads
seqtabHC.f2 = seqtabHC.f[,lowcountidxHC]
dim(seqtabHC.f2)
write.table(data.frame(ASV=colnames(seqtabHC.f2), t(seqtabHC.f2)), file='01-filtered-seqtab_HC.tsv', sep='\t', row.names = F)
  
# Filters LC:
failedidxLC = rowSums(seqtabLC >= 10) > 0
commonidxLC = colSums(seqtabLC > 1)/nrow(seqtabLC) < .5  # throw out ASVs that show up in >2 samples
seqtabLC.f = seqtabLC[failedidxLC, commonidxLC]
seqtabLC.f.pct = 100*seqtabLC.f/rowSums(seqtabLC.f)
lowcountidxLC = colSums(seqtabLC.f.pct > 30) > 0 # Throw out ASVs that do not get support from at least 30% of reads
seqtabLC.f2 = seqtabLC.f[,lowcountidxLC]
dim(seqtabLC.f2)
write.table(data.frame(ASV=colnames(seqtabLC.f2), t(seqtabLC.f2)), file='01-filtered-seqtab_LC.tsv', sep='\t', row.names=F)

## Convert HC ASV table to output table:
options(stringsAsFactors = F)
HC.out = data.frame()
for(s in rownames(seqtabHC.f2)){
  r = seqtabHC.f2[s, ]  
  trimmerID = sampleIDs$TrimmerID[match(s, sampleIDs$InlineIndexDescription)]
  for(asv in names(which(r/sum(r) > .3))){
    HC.out = rbind(HC.out, c(plate, s, trimmerID, r[asv], signif(100*r[asv]/sum(seqtabHC[s,]), 3), asv))
  }
}
colnames(HC.out) = c("plate", "SMARTindex", "TrimmerID", "ASVcount", "PctSupport", "HeavyChain")
readr::write_tsv(HC.out, paste0(outp, plate, "_", "HeavyChain.tsv"))

## Convert LC ASV table to output table:
options(stringsAsFactors = F)
LC.out = data.frame()
for(s in rownames(seqtabLC.f2)){
  r = seqtabLC.f2[s, ]  
  trimmerID = sampleIDs$TrimmerID[match(s, sampleIDs$InlineIndexDescription)]
  for(asv in names(which(r/sum(r) > .3))){
    LC.out = rbind(LC.out, c(plate, s, trimmerID, r[asv], signif(100*r[asv]/sum(seqtabLC[s,]), 3), asv))
  }
}
colnames(LC.out) = c("plate", "SMARTindex", "TrimmerID", "ASVcount", "PctSupport", "LightChain")
readr::write_tsv(LC.out, paste0(outp, plate, "_", "LightChain.tsv"))



```


