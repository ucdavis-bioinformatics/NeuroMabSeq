---
title: "Hybridoma Amplicon Report"
plate: "`r tail(unlist(strsplit(getwd(), '/')), 2)[1]`"
author: Sam Hunter
date: "`r format(Sys.time(), '%d %B, %Y')`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
#knitr::opts_knit$set(root.dir = '/bio/CoreWork/2019.11.18-Trimmer-Hybridoma-Seq/2020-02-07-run-rerun_of_27-samples-SMARTPCR')
library(dada2)
library(kableExtra)
library(ggplot2)
library(stringr)
library(msaR)
library(DECIPHER)
options(stringsAsFactors=F)
```

### Setup File and Sample Names

```{r Load Data}
# Load plate translation:
outp = './'
#sampleIDs = read.table("../samplesheet.tsv", header=T, as.is=T, sep='\t')

submission = 'b51b81e9ee80'
plate = "plate2"

sampleData = read.table(paste("https://submissions.dnatech.ucdavis.edu/server/api/submissions/", 
                              submission, "/download/?format=tsv&data=samples", sep=''), sep='\t', header=T, as.is=T)

#path = "../01-PrimerTrim-keep-aberrant-LC/"
path = "../01-PrimerTrim/"

list.files(path, pattern = "*_SE.fastq.gz")

# Get list of files
hcfiles = sort(list.files(path, pattern="*_10-REV-HC1_SE.fastq.gz", full.names=T))
lcfiles = sort(list.files(path, pattern="*_6-REV-LC_SE.fastq.gz", full.names=T))

# Get SMARTPCR index identifier:
hcindexes = str_remove(basename(hcfiles), "_10-REV-HC1_SE.fastq.gz")
lcindexes = str_remove(basename(lcfiles), "_6-REV-LC_SE.fastq.gz")

# Filter reads that have errors:
# filtHC = file.path(path, 'filtered', paste0(hcindexes, "_HC_filt.fastq.gz"))
# filtLC = file.path(path, 'filtered', paste0(lcindexes, "_LC_filt.fastq.gz"))

# outHC = filterAndTrim(hcfiles, filtHC, maxEE = Inf, maxN=0, compress=TRUE, multithread=TRUE, truncQ=0)
# gc()
# outLC = filterAndTrim(lcfiles, filtLC, maxEE = 3, maxN=0, compress=TRUE, multithread=TRUE, truncQ=0)
# gc()

```

```{r learningErrors}
# Learn Error Rates
errHC = learnErrors(hcfiles, multithread = TRUE)
errLC = learnErrors(lcfiles, multithread = TRUE)
gc()

```

Plot error rates:

```{r Plot HC Error Model}
plotErrors(errHC, nominalQ = TRUE)

```

```{r Plot LC Error Models}
plotErrors(errLC, nominalQ = TRUE)

```


### Infer ASVs

```{r InferASV}

sp0 = "CAGCATCCTCTCTTCCAGCTCTCAGAGATGGAGACAGACACACTCCTGTTATGGGTACTGCTGCTCTGGG
TTCCAGGTTCCACTGGTGACATTGTGCTGACACAGTCTCCTGCTTCCTTAGCTGTATCTCTGGGGCAGAG
GGCCACCATCTCATACAGGGCCAGCAAAAGTGTCAGTACATCTGGCTATAGTTATATGCACTGGAACCAA
CAGAAACCAGGACAGCCACCCAGACTCCTCATCTATCTTGTATCCAACCTAGAATCTGGGGTCCCTGCCA
GGTTCAGTGGCAGTGGGTCTGGGACAGACTTCACCCTCAACATCCATCCTGTGGAGGAGGAGGATGCTGC
AACCTATTACTGTCAGCACATTAGGGAGCTTACACGTTCGGAGGGGGGACCAAGCTGGAAATAAAACGGG
CTGATGCTGCACCAACTGTATCCA"
sp0 = gsub('\n', '', sp0)

# Do denoising for HC:
dadaHC = dada(hcfiles, err=errHC, multithread=T, pool=T, priors=sp0, OMEGA_A=1e-60, OMEGA_C=0)
names(dadaHC) = hcindexes

# Do denoising for LC:
dadaLC = dada(lcfiles, err=errLC, multithread=T, pool=T, priors=sp0, OMEGA_A=1e-60, OMEGA_C=0)
names(dadaLC) = lcindexes

gc()

# Construct sequence tables
seqtabHC <- makeSequenceTable(dadaHC)
seqtabLC <- makeSequenceTable(dadaLC)

# Write some non-filtered tables for tracking and analysis
dim(seqtabHC)
seqtabHC.tab = data.frame(ASV=colnames(seqtabHC), t(seqtabHC))
seqtabHC.tab = seqtabHC.tab[order(rowSums(t(seqtabHC)), decreasing = T), ]
write.table(seqtabHC.tab, file='00-no-filter-seqtab_HC.tsv', sep='\t', row.names=F)

dim(seqtabLC)
seqtabLC.tab = data.frame(ASV=colnames(seqtabLC), t(seqtabLC))
seqtabLC.tab = seqtabLC.tab[order(rowSums(t(seqtabLC)), decreasing = T), ]
write.table(seqtabLC.tab, file='00-no-filter-seqtab_LC.tsv', sep='\t', row.names=F)

```

```{r Process HC}
############ Filter ASVs and samples ######## 
# Rows are ASVs, cols are samples:
tmp = seqtabHC
colnames(tmp) = paste0('H', 1:ncol(tmp))

# Filters HC:
failedidxHC = rowSums(seqtabHC >= 10) > 0 # throw out samples that have no ASV with at least 10 reads of support 
commonidxHC = colSums(seqtabHC > 5)/nrow(seqtabHC) < .5   # throw out ASVs that show up in > 50% of samples 
seqtabHC.f = seqtabHC[failedidxHC, commonidxHC]
seqtabHC.f.pct = 100*seqtabHC.f/rowSums(seqtabHC.f)
lowcountidxHC = colSums(seqtabHC.f.pct > 30) > 0 # Throw out ASVs that do not get support from at least 30% of reads
seqtabHC.f2 = seqtabHC.f[,lowcountidxHC]
dim(seqtabHC.f2)
write.table(data.frame(ASV=colnames(seqtabHC.f2), t(seqtabHC.f2)), file='01-filtered-seqtab_HC.tsv', sep='\t', row.names = F)


## Convert HC ASV table to output table:
options(stringsAsFactors = F)
HC.out = data.frame()
for(s in rownames(seqtabHC.f2)){
  r = seqtabHC.f2[s, ]
  MabID = sampleData$trimmer_id[match(s, sampleData$inline_index_name)]
  for(asv in names(which(r/sum(seqtabHC[s,]) > .2))){ 
    # Only keep ASVs with at least 30% read support:
    HC.out = rbind(HC.out, c(plate, s, MabID, r[asv], signif(100*r[asv]/sum(seqtabHC[s,]), 3), sum(seqtabHC[s,]), "Illumina", asv))
  }
}
colnames(HC.out) = c("plate", "SMARTindex", "MabID", "ASVsupport", "PctSupport", "TotalReads", "Sequencing", "HeavyChain")

# Find any ASVs reported more than once and note the duplicate
HC.out$DuplicatedIn =apply(HC.out, 1, function(x){ paste(setdiff(HC.out$MabID[HC.out$HeavyChain %in% x['HeavyChain']], x['MabID']), collapse=', ')})
readr::write_tsv(HC.out, paste0(outp, plate, "_", "HeavyChain.tsv"))

```


```{r, Process LC }

# Filters LC:
failedidxLC = rowSums(seqtabLC >= 10) > 0
commonidxLC = colSums(seqtabLC > 5)/nrow(seqtabLC) < .5  # throw out ASVs that show up in >50% samples with at least 5 reads
seqtabLC.f = seqtabLC[failedidxLC, commonidxLC]
seqtabLC.f.pct = 100*seqtabLC.f/rowSums(seqtabLC.f)
lowcountidxLC = colSums(seqtabLC.f.pct > 30) > 0 # Throw out ASVs that do not get support from at least 30% of reads
seqtabLC.f2 = seqtabLC.f[,lowcountidxLC]
dim(seqtabLC.f2)
write.table(data.frame(ASV=colnames(seqtabLC.f2), t(seqtabLC.f2)), file='01-filtered-seqtab_LC.tsv', sep='\t', row.names=F)

## Convert LC ASV table to output table:
options(stringsAsFactors = F)
LC.out = data.frame()
for(s in rownames(seqtabLC.f2)){
  r = seqtabLC.f2[s, ]  
  MabID = sampleData$trimmer_id[match(s, sampleData$inline_index_name)]
  for(asv in names(which(r/sum(seqtabLC[s,]) > .2))){
    LC.out = rbind(LC.out, c(plate, s, MabID, r[asv], signif(100*r[asv]/sum(seqtabLC[s,]), 3), sum(seqtabLC[s,]), "Illumina", asv))
  }
}
colnames(LC.out) = c("plate", "SMARTindex", "MabID", "ASVcount", "PctSupport", "TotalReads", "Sequencing", "LightChain")
# Find any ASVs reported more than once and note the duplicate
LC.out$DuplicatedIn = apply(LC.out, 1, function(x){ paste(setdiff(LC.out$MabID[LC.out$HeavyChain %in% x['LightChain']], x['MabID']), collapse=', ')})
readr::write_tsv(LC.out, paste0(outp, plate, "_", "LightChain.tsv"))

```

### Putative aberrant LC alignment
```{r plotalignment}
# Plot alignments of commonidxLC (are these aberrant?)
# So far haven't figure out how to do this in a loop, so just plot the one with the most total reads:
putative_aberrant = names(sort(colSums(seqtabLC[,!commonidxLC, drop=F]), decreasing=T))[1]
dna = DNAStringSet(c(sp0, putative_aberrant))  
names(dna) = c("AberrantLC", "PutativeALC") 

alignedDNA = AlignSeqs(dna, verbose=F)

msaR(alignedDNA, menu=F, height=100, overviewbox = F, conservation=T, )

```
\
\
\
```{r summary table}
## Build Status Table for all samples:
status.out = data.frame(sampleData,
                  LCs.Reported = as.vector(table(LC.out$MabID)[sampleData$trimmer_id]),
                  HCs.Reported = as.vector(table(HC.out$MabID)[sampleData$trimmer_id])
)
readr::write_tsv(status.out, paste0(outp, plate, "_", "SampleStatus.tsv"))

kable(status.out) %>%
  kable_styling("striped", full_width = F) %>%
  row_spec(0, angle = 0)

```


